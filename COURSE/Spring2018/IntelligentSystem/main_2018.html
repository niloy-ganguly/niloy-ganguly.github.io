<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
<html>
<head>
  <Title> CS 33006 Computer Networks </Title>   
</head>

<body bgcolor="#000000" text="#ffffcc" link="#ffff00" vlink="#ddcc55" alink="#ff9999">

<center>
<br>
<br>
<H1><font size=+2>CS 60036 Intelligent Systems </font></H1>
<H3>(Spring Semester 2018)</H3>
<b>Theory</b><br>  <b>Niloy Ganguly (NG)</b> niloy {AT} cse.iitkgp.ernet.in<br>

<br><br>

<b>Teaching Assistants</b><p><b>Abhijnan Chakraborty -- chakraborty [DOT] 
abhijnan {AT} gmail.com</b></p>
<p><b>Bidisha Samanta -- bidisha [DOT] samanta {AT} gmail.com</b><br> 
<br><br>
</p>
<p>
</center>
<hr>
<h3>Notices</h3>
&nbsp;<hr>
<a href="#Essentials"><H3>Theory</H3></a>
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<a href="#Essentials">Class Room/Hour</a><br>
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<a href="#Books">Course Overview</a><br>
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<a href="#Assessment">Evaluation</a><br>
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<a href="#Lecture">Lectures</a><br>
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<a href="#Assignment">Assignments</a><br>
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<a href=student.pdf>Students List</a><p>&nbsp;</p>
 &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<br>
<hr>

<a name="Essentials"><H2>Class Room/Hours</H2></a>
  Lectures : Wed - 3, Thu&nbsp; - 2, Fri - 4, 5 (fourth class will be taken as 
per need)<br>
  Room      :  302<br>
  Units : 3-0-0<br>
  Credits    :  3 <br>
  Contact    :  Room #313  (CSE), Phone 83460<br>


<a name="Books"><H2>Course Overview</H2>
<p>Intelligent systems have been traditionally designed to solve problems where 
human solution was inefficient and would take a lot of man-hours. The tasks were 
mostly repetitive and can be specified by the humans either through well-formed 
rules or through feature engineering in labelled data. However, decision making 
was still an exclusive domain of human intelligence.<br>
<br>
Recent years have seen a lot of changes in the above scenario. Many hitherto 
human decision domains (such as evaluating creditworthiness, judicial decisions, 
choosing electoral winners etc.) are increasingly becoming algorithmic. Though, 
in many cases, algorithmic automation might reduce the complexity of the 
problem, it is also evoking mistrust by ‘hiding the machine.’ Also, often while 
designing, human bias against people having particular race, gender or economic 
status are unknowingly being transferred into machine intelligence. Hence, with 
the booming popularity of the Artificial Intelligence, there is a need to 
address the very real problems with AI today like discrimination, lack of 
fairness and trust. In this course, we will study how these issues creep into 
the design of AI systems, and also learn mechanisms using which we can mitigate 
discrimination and design systems which are fair and trustworthy.<br>
<br>
<b><font size="4">Topics:</font></b><br>
<br>
Introduction to intelligent systems<br>
Algorithmic bias and discrimination<br>
Discovering discrimination in historical decision records<br>
Preventing discrimination<br>
Fairness aware data mining<br>
Rethinking fairness as social choice <br>
Intelligent electoral systems<br>
Impossibility of fairness axioms<br>
Strategic Manipulation in elections<br>
Coping with Strategic Manipulation<br>
Information and Communication in Voting<br>
Multiwinner Voting Rules<br>
Introduction to Fair Division <br>
Fairness and Efficiency Criteria<br>
Divisible Goods: Cake-Cutting Procedures <br>
Indivisible Goods: Combinatorial Optimisation<br>
Fair Allocation<br>
Introduction to Blockchain<br>
Blockchain architecture<br>
Blockchain use-case - Bitcoin and its architecture<br>
Weakness of blockchain technology<br>
Future directions in trustworthy computations<br>
&nbsp;</p><hr>
<p><br>
<b><font size="4">Books:</font></b><br>
1. Blockchain for dummies<br>
https://www-01.ibm.com/common/ssi/cgi-bin/ssialias?htmlfid=XIM12354USEN <br>
<br>
2. Handbook of Computational Social Choice - Ariel Procaccia<br>
http://procaccia.info/papers/comsoc.pdf<br>
&nbsp;</p><hr></a>
<p>Reading from the Web:</p>
<p>1.
<a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">
https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing</a></p>
<p>2. <a href="https://medium.com/@mrtz/how-big-data-is-unfair-9aa544d739de">
https://medium.com/@mrtz/how-big-data-is-unfair-9aa544d739de</a></p>
<p>3.
<a href="https://obamawhitehouse.archives.gov/sites/default/files/microsites/ostp/2016_0504_data_discrimination.pdf">
https://obamawhitehouse.archives.gov/sites/default/files/microsites/ostp/2016_0504_data_discrimination.pdf</a></p>
<p><br>
<b><font size="4">Papers:</font></b><br>
1. I Zliobait&#279; (2015): <a href="PPT_2018/1511.00148.pdf">A survey on measuring 
indirect discrimination in machine learning</a>.<br>
2. Richard S. Zemel, Yu Wu, Kevin Swersky, Toniann Pitassi, and Cynthia Dwork. 
2013: <a href="PPT_2018/icml-final.pdf">Learning Fair Representations</a>. In 
Proc. of the 30th Int. Conf. on Machine Learning. 325–333.<br>
3. D. Pedreschi, S. Ruggieri, F. Turini: <a href="PPT_2018/p126-pedreschi.pdf">A 
Study of Top-K Measures for Discrimination Discovery</a>. SAC 2012. <br>
4. Romei, A. and Ruggieri, S., 2014. <a href="PPT_2018/10.1.1.446.5341.pdf">A 
multidisciplinary survey on discrimination analysis</a>. The Knowledge 
Engineering Review, 29(5), pp.582-638.</p>
<p>5. Ruggieri, S., Pedreschi, D. and Turini, F., 2010.
<a href="PPT_2018/a9-ruggieri.pdf">Data mining for discrimination discovery</a>. 
ACM Transactions on Knowledge Discovery from Data (TKDD), 4(2), p.9.</p>
<p>6. Pedreschi, D., Ruggieri, S. and Turini, F., 2009, June.
<a href="PPT_2018/p157-pedreschi.pdf">Integrating induction and deduction for 
finding evidence of discrimination</a>. In Proceedings of the 12th International 
Conference on Artificial Intelligence and Law (pp. 157-166). ACM.</p>
<p>7. Luong, B.T., Ruggieri, S. and Turini, F., 2011, August.
<a href="PPT_2018/p502-luong.pdf">k-NN as an implementation of situation testing 
for discrimination discovery and prevention</a>. In Proceedings of the 17th ACM 
SIGKDD international conference on Knowledge discovery and data mining (pp. 
502-510). ACM.</p>
<p>8. Mancuhan, K. and Clifton, C., 2014. <a href="PPT_2018/10.1007.pdf">
Combating discrimination using bayesian networks</a>. Artificial intelligence 
and law, 22(2), pp.211-238.</p>
<p>9. Francesco Bonchi1; Sara Hajian, Bud Mishra, Daniele Ramazzotti,
<a href="PPT_2018/causal-bonchi.pdf">Exposing the Probabilistic Causal Structure 
of Discrimination</a>, International Journal of Data Science and Analytics &gt; 
Issue 1/2017</p>
<p>10. Salvatore Ruggieri, Sara Hajian, Faisal Kamiran, and Xiangliang Zhang,
<a href="PPT_2018/discrimination-privacy-ruggieri.pdf">Anti-discrimination 
Analysis Using Privacy Attack Strategies</a>, ECML-PKDD 2014</p>
<p>11. M. Feldman, S.A. Friedler, J. Moeller, C. Scheidegger, and S. 
Venkatasubramanian, <a href="https://arxiv.org/abs/1412.3756">Certifying and 
removing disparate impact.</a> In KDD, pp. 259-268, 2015.</p>
<p><br>
12. F. Kamiran and T. Calders. <a href="PPT_2018/DataPreprocessClass.pdf">Data 
preprocessing techniques for classification without discrimination</a>. In 
Knowledge and Information Systems (KAIS), 33(1), 2012.</p>
<p>13. A methodology for direct and indirect discrimination prevention in data 
mining<br>
S. Hajian and J. Domingo-Ferrer. In IEEE Transactions on Knowledge and Data 
Engineering (TKDE), 25(7), 2013.<br>
http://ieeexplore.ieee.org/document/6175897/<br>
<br>
14. <a href="https://people.mpi-sws.org/~mzafar/papers/disparate_impact.pdf">Fairness Constraints: Mechanisms for Fair Classification</a> <br>
M. B. Zafar, I. Valera, M. Gomez Rodriguez and K. P. Gummadi <br>
AISTATS 2017, Fort Lauderdale, FL, April 2017.<br>
<br>
15. 
<a href="https://people.mpi-sws.org/~mzafar/papers/disparate_mistreatment.pdf">Fairness Beyond Disparate Treatment &amp; Disparate Impact: Learning 
Classification without Disparate Mistreatment</a> <br>
M. B. Zafar, I. Valera, M. Gomez Rodriguez and K. P. Gummadi <br>
WWW 2017, Perth, Australia, April 2017.<br>
<br>
16. 
<a href="https://people.mpi-sws.org/~mzafar/papers/preferential_fairness_nips2017.pdf">From Parity to Preference-based Notions of Fairness in Classification</a> <br>
M. B. Zafar, I. Valera, M. Gomez Rodriguez, K. P. Gummadi and A. Weller <br>
NIPS 2017, Long Beach, CA, December 2017.<br>
<br>
17. 
<a href="https://people.mpi-sws.org/~mzafar/papers/process_fairness_aaai2018.pdf">Beyond Distributive Fairness in Algorithmic Decision Making: Feature 
Selection for Procedurally Fair Learning</a> <br>
N. Grgi&#263;-Hla&#269;a, M. B. Zafar, K. P. Gummadi and A. Weller <br>
AAAI 2018, New Orleans, LA, February 2018.<br>
<br>
18. I. Zliobaite, F. Kamiran and T. Calders. <a href="PPT_2018/conditional.pdf">
Handling conditional discrimination</a>. In ICDM, pp. 992- 1001, 2011.</p>
<p>19. Ribeiro, M.T., Singh, S. and Guestrin, C., 2016, August.
<a href="PPT_2018/rfp0573-ribeiroA.pdf">Why should i trust you?: Explaining the 
predictions of any classifier.</a> In Proceedings of the 22nd ACM SIGKDD 
International Conference on Knowledge Discovery and Data Mining (pp. 1135-1144). 
ACM.</p>
<p>20. C. Dwork, M. Hardt, T. Pitassi, O. Reingold and R. S. Zemel.
<a href="PPT_2018/Fairnessthroughawareness.pdf">Fairness through awareness</a>. 
In ITCS 2012, pp. 214-226, 2012.</p>
<p>21. Bolukbasi, T., Chang, K.W., Zou, J.Y., Saligrama, V. and Kalai, A.T., 
2016. <a href="PPT_2018/cmu13-fairness.pdf">Man is to computer programmer as 
woman is to homemaker? debiasing word embeddings.</a> In Advances in Neural 
Information Processing Systems (pp. 4349-4357).</p>
<p>22. Chierichetti, F., Kumar, R., Lattanzi, S. and Vassilvitskii, S., 2017.
<a href="PPT_2018/7088-fair-clustering-through-fairlets.pdf">Fair Clustering 
Through Fairlets.</a> In Advances in Neural Information Processing Systems (pp. 
5036-5044).</p><hr>
<p><a name="Assessment">Evaluation</p></a>
        Teacher's Assessment : 35 <br>
                <!-- &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Attendance     : 4<br>
                &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Extra              : 4<br><br>-->
        <a href="MidSem-Question-Paper-2018.pdf">Mid-sem</a> : 25<br>
        <a href="EndSem-Question-Paper-2018.pdf">End-sem</a> : 40<hr>
<p><br>


</p>


<H2><a name="Lecture" href="PPT/Introduction.ppt">Lectures</a></H2>
<p>1. <a href="PPT_2018/algo_bias_discrmination.pptx">Discrimination Discovery
</a></p>
<p>2. <a href="PPT_2018/Fairness%20aware%20data%20mining_new.pdf">Fairness Aware 
ML</a></p>
<p>3. <a href="PPT_2018/why_should_i_trust_ppt.pdf">Why should i trust you?</a></p>
<p>4.
<a href="PPT_2018/Fair%20Clustering%20through%20Fairlets%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20(%20NIPS%202017).pptx">
Fair Clustering Through Fairlets</a></p>
<p>5. <a href="PPT_2018/aistats_www.pdf">Fairness Constraints</a></p>
<p>6. <a href="PPT_2018/nips.pdf">Preference Based Fairness</a></p>
<p>7.<a href="PPT_2018/procedural_fairness_AAAI_2018.pdf"> Procedural Fairness</a></p>
<p>8. <a href="PPT_2018/cmu13-fairness.pdf">Fairness through awareness</a>.</p>
<hr>
<p>&nbsp;</p>
  <a name="Assignment"><H2>Assignments</a><br>

</H2>
<a name="Lab-Assign">

<center>
<p align="left">&nbsp;</p>

<center>
<p align="left">&nbsp;</p> </center> </center> </a> 